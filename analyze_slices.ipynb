{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctSliceDataset(Dataset):\n",
    "    def __init__(self, data_dir, slice_min=80, slice_max=120):\n",
    "        assert 0 <= slice_min <= slice_max < 200\n",
    "        self.slice_min = slice_min\n",
    "        self.slice_max = slice_max\n",
    "        \n",
    "        pos_dir = os.path.join(data_dir, 'pos')\n",
    "        pos_paths = [os.path.join(pos_dir, f) for f in os.listdir(pos_dir)]\n",
    "        \n",
    "        neg_dir = os.path.join(data_dir, 'neg')\n",
    "        neg_paths = [os.path.join(neg_dir, f) for f in os.listdir(neg_dir)]\n",
    "        \n",
    "        self.cube_paths = pos_paths + neg_paths\n",
    "        self.labels = [1.] * len(pos_paths) + [0.] * len(neg_paths)\n",
    "        \n",
    "        self.transforms = T.Compose([\n",
    "            T.Resize([200, 200]),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        assert len(self.labels) == len(self.cube_paths)\n",
    "        print(f'Number of cubes: {len(self.labels)}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        cube = np.load(self.cube_paths[i])\n",
    "        label = self.labels[i]\n",
    "        \n",
    "        slice_idx = random.randint(self.slice_min, self.slice_max)\n",
    "        \n",
    "        slice_ = cube[:, :, slice_idx]\n",
    "        img = Image.fromarray(slice_)\n",
    "        return torch.tensor(self.transforms(img)), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        return x.view(N, -1)\n",
    "\n",
    "class OctSliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OctSliceNet, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            Flatten(),\n",
    "            nn.Linear(8464, 1)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, slice_):\n",
    "        return self.cnn(slice_).squeeze(dim=1)\n",
    "    \n",
    "    def train_step(self, slice_, targets):\n",
    "        logits = self(slice_)\n",
    "        loss = self.loss_fn(logits, targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, loader, verbose=True):\n",
    "    start = time.time()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    all_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(loader):\n",
    "            logits = net(X.to(device))\n",
    "            loss = net.loss_fn(logits, y.to(device))\n",
    "            all_logits.extend(list(logits.cpu().numpy()))\n",
    "            all_labels.extend(list(y))\n",
    "            all_losses.append(loss.item())\n",
    "\n",
    "    val_loss = statistics.mean(all_losses)\n",
    "    auprc = average_precision_score(all_labels, all_logits)\n",
    "    auroc = roc_auc_score(all_labels, all_logits)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Average precision score: {auprc}')\n",
    "        print(f'AUROC: {auroc}')\n",
    "        print(f'Validation loss (approximate): {val_loss}')\n",
    "        print(f'Elapsed: {time.time() - start}')\n",
    "    return val_loss, auprc, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(constants.PROCESSED_DATA_PATH, 'train')\n",
    "val_dir = os.path.join(constants.PROCESSED_DATA_PATH, 'val')\n",
    "test_dir = os.path.join(constants.PROCESSED_DATA_PATH, 'test')\n",
    "\n",
    "def train_with_slice(slice_idx, num_epochs=10, verbose=True):\n",
    "    print('==============')\n",
    "    print(f'Training with slice index: {slice_idx}')\n",
    "    \n",
    "    train_dataset = OctSliceDataset(train_dir, slice_min=slice_idx, slice_max=slice_idx)\n",
    "    val_dataset = OctSliceDataset(val_dir, slice_min=slice_idx, slice_max=slice_idx)\n",
    "    test_dataset = OctSliceDataset(test_dir, slice_min=slice_idx, slice_max=slice_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=8)\n",
    "\n",
    "    net = OctSliceNet().to(device)\n",
    "\n",
    "    if verbose: print('------ Evaluating ------')\n",
    "    evaluate(net, val_loader, verbose)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if verbose: print(f'====== Epoch {epoch} ======')\n",
    "        losses = []\n",
    "        for X, y in train_loader:\n",
    "            loss = net.train_step(X.to(device), y.to(device))\n",
    "            loss = loss.item()\n",
    "            losses.append(loss)\n",
    "        train_loss = statistics.mean(losses)\n",
    "        if verbose: print(f'Train loss (approximate): {train_loss}')\n",
    "\n",
    "        if verbose: print('------ Evaluating ------')\n",
    "        val_loss, auprc, auroc = evaluate(net, val_loader, verbose)\n",
    "        \n",
    "    return train_loss, val_loss, auprc, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idxs = [0, 25, 50, 75, 100, 125, 150, 175, 199]\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "auprcs = []\n",
    "aurocs = []\n",
    "\n",
    "for slice_idx in slice_idxs:\n",
    "    train_loss, val_loss, auprc, auroc = train_with_slice(slice_idx)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    auprcs.append(auprc)\n",
    "    aurocs.append(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slice_idxs, train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slice_idxs, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slice_idxs, auprcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slice_idxs, aurocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
